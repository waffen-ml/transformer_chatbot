{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "52yEKz5Ga-QE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owCZPzWjpXiI",
        "outputId": "a6de2b72-fff6-4108-807f-f40ebe0ff4a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 30\n",
        "BATCH_SIZE = 400\n",
        "D_MODEL = 256\n",
        "D_HID = 512\n",
        "N_HEAD = 4\n",
        "N_LAYERS = 4\n",
        "DROPOUT = 0.1\n",
        "\n",
        "PAD = '<pad>'\n",
        "START = '<start>'\n",
        "END = '<end>'\n",
        "UNK = '<unk>'"
      ],
      "metadata": {
        "id": "-9eWzOg4igop"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('lines.txt', encoding='utf-8') as f:\n",
        "    lines_raw = f.read().splitlines()\n",
        "\n",
        "lines = {}\n",
        "\n",
        "for l in lines_raw:\n",
        "    s = l.split(' +++$+++ ')\n",
        "    lines[s[0]] = s[-1]"
      ],
      "metadata": {
        "id": "z-z4JmQ6cemb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('conversations.txt', encoding='utf-8') as f:\n",
        "    conv_raw = f.read().splitlines()\n",
        "\n",
        "conversations = []\n",
        "\n",
        "for conv in conv_raw:\n",
        "    arr = eval(conv.split(' +++$+++ ')[-1])\n",
        "    arr = [lines[i] for i in arr[:2]]\n",
        "    if len(arr) != 2:\n",
        "        continue\n",
        "    conversations.append(arr)"
      ],
      "metadata": {
        "id": "_XcwA6eKc2Ep"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversations[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q8DDACMdx8y",
        "outputId": "bcabbc50-85f3-4360-ac16-35ef326ae012"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
              " \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversations = [\n",
        "    ['hello, how are you?', 'hello, i am fine'],\n",
        "    ['hello', 'hello'],\n",
        "    ['how are you?', 'i am fine']\n",
        "]"
      ],
      "metadata": {
        "id": "8ZR1h3DJ7LCX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "tokenized_conv = [[tokenizer(line) for line in unit] for unit in tqdm(conversations)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8jzjcgld3FR",
        "outputId": "23abd1f0-c275-4134-a02f-2ef55e548ea5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83097/83097 [00:02<00:00, 34141.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squeezed_conv = []\n",
        "for tc in tokenized_conv:\n",
        "    squeezed_conv += tc"
      ],
      "metadata": {
        "id": "8PmiXFRPhvJ5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab_from_iterator(squeezed_conv, specials=[PAD, START, END, UNK])\n",
        "vocab.set_default_index(vocab[UNK])"
      ],
      "metadata": {
        "id": "xLdUv8Ipf1TA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "END_PUNCT = list('.?!') + ['...']\n",
        "INNER_PUNCT = ',:;-\"'\n",
        "\n",
        "\n",
        "def random_amount(max_amount, v, f=0.5):\n",
        "    return min(random.randint(0, math.ceil(1.25 * v * max_amount ** f)), max_amount)\n",
        "\n",
        "\n",
        "def random_indices(length, v, f=0.5, reverse=False):\n",
        "    amount = random_amount(length, v, f)\n",
        "    try:\n",
        "        return sorted(random.sample(range(length), amount), reverse=reverse)\n",
        "    except:\n",
        "        print(length, amount)\n",
        "        raise Exception('no. fuck')\n",
        "\n",
        "\n",
        "def binrand(p):\n",
        "    return p > random.random()\n",
        "\n",
        "\n",
        "class Shuffler:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def swap(self, t, v, max_strength=3):\n",
        "        strength = math.ceil(max_strength * v)\n",
        "        for i in random_indices(len(t) - 1, v, f=0.4):\n",
        "            s_bef = min(strength, i)\n",
        "            s_aft = min(strength, len(t) - i - 1)\n",
        "            diff = random.randint(0, s_bef + s_aft) - s_bef\n",
        "            diff = diff if diff else 1\n",
        "            t[i], t[i + diff] = t[i + diff], t[i]\n",
        "\n",
        "    def double(self, t, v):\n",
        "        for offset, i in enumerate(random_indices(len(t), v)):\n",
        "            t.insert(offset + i, t[offset + i])\n",
        "\n",
        "    def add(self, t, v):\n",
        "        for offset, i in enumerate(random_indices(len(t), v)):\n",
        "            array = INNER_PUNCT if binrand(0.75) or not self.vocab else self.vocab\n",
        "            t.insert(offset + i, random.choice(array))\n",
        "\n",
        "    def add_end(self, t, v):\n",
        "        if not binrand(v):\n",
        "            return\n",
        "        t.append(random.choice(END_PUNCT))\n",
        "\n",
        "    def shuffle(self, t, v):\n",
        "\n",
        "        if not t:\n",
        "            return t\n",
        "\n",
        "        self.double(t, v)\n",
        "        self.add(t, v)\n",
        "        self.add_end(t, v)\n",
        "        self.swap(t, v)\n",
        "\n",
        "        return t\n",
        "\n",
        "\n",
        "def produce_shuffled(tokens, steps, vocab=None, max_shuffle=1):\n",
        "    shuffler = Shuffler(vocab or [])\n",
        "    v = torch.linspace(0, max_shuffle, steps)\n",
        "    result = []\n",
        "\n",
        "    for tline in tokens:\n",
        "        result += [shuffler.shuffle(tline.copy(), vi) for vi in v]\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "3-tUeSELGeTg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_conv = []\n",
        "\n",
        "for q, a in tqdm(tokenized_conv):\n",
        "    shuffled = produce_shuffled([q], 3)\n",
        "    shuffled_conv += [[sh, a] for sh in shuffled]\n",
        "\n",
        "len(tokenized_conv), len(shuffled_conv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EOpMf80G8Zw",
        "outputId": "4a329d22-1be5-496d-bb62-a41618c64e3a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83097/83097 [00:26<00:00, 3168.83it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83097, 249291)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_conv = shuffled_conv"
      ],
      "metadata": {
        "id": "TxjUxPgCHzsY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_conv = [[vocab(line) for line in conv] for conv in tokenized_conv]"
      ],
      "metadata": {
        "id": "gKOv53udhGZg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_qna_sequence(q, a):\n",
        "    s = [vocab[START]] + q + [vocab[END], vocab[START]]\n",
        "    s = [vocab[PAD]] * (SEQ_LEN - len(s)) + s[-SEQ_LEN:] + a + [vocab[END]]\n",
        "    return s"
      ],
      "metadata": {
        "id": "8RGFNrmsjbtJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna = []\n",
        "\n",
        "for conv in tqdm(num_conv):\n",
        "    seq = make_qna_sequence(conv[0], conv[1])\n",
        "    qna.append(seq)\n",
        "\n",
        "len(qna)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsuifj8MlVMZ",
        "outputId": "df998302-8404-486b-e2d8-351ef89cf4e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 249291/249291 [00:02<00:00, 114952.40it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249291"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.borders = []\n",
        "    self.length = 0\n",
        "\n",
        "    for qna in data:\n",
        "      self.borders.append(self.length)\n",
        "      self.length += len(qna) - SEQ_LEN\n",
        "\n",
        "  def _bin_search(self, w):\n",
        "      l, r = 0, len(self.borders)\n",
        "      while r - l > 1 and w != self.borders[l]:\n",
        "          i = (l + r) // 2\n",
        "          if w < self.borders[i]:\n",
        "              r = i\n",
        "          else:\n",
        "              l = i\n",
        "      return l\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    arr_id = self._bin_search(i)\n",
        "    j = i - self.borders[arr_id]\n",
        "    qna = self.data[arr_id]\n",
        "    return torch.tensor(qna[j:j + SEQ_LEN]), qna[j + SEQ_LEN]"
      ],
      "metadata": {
        "id": "GxiCrqf0iRfQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[0, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "GAeoP2Z9m79f"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(D_MODEL, SEQ_LEN, DROPOUT).to(device)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(D_MODEL, N_HEAD, D_HID, DROPOUT, batch_first=True).to(device)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, N_LAYERS).to(device)\n",
        "        self.embedding = nn.Embedding(vocab_size, D_MODEL).to(device)\n",
        "        self.linear = nn.Linear(D_MODEL * SEQ_LEN, vocab_size).to(device)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src) * math.sqrt(D_MODEL)\n",
        "        #src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.reshape(-1, SEQ_LEN * D_MODEL)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "IpllnabJnijP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(MyDataset(qna[:50_000]), batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "4aJDGvaw3fud"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel(len(vocab), device=device)"
      ],
      "metadata": {
        "id": "0EBYZkYpqVH3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.load_state_dict(torch.load('model.pt'))"
      ],
      "metadata": {
        "id": "jkYJGHWE8jxT"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "EhhzcV4StfnO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "jmVag21i5XgU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('-' * 6, epoch, '-' * 6)\n",
        "\n",
        "    train_loss = 0\n",
        "\n",
        "    for (x, y) in tqdm(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    print(f'Train loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "8rpByYpJtjkW",
        "outputId": "9f8c3ef2-ac9a-4e80-9dd6-44643a0997db"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ 0 ------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1784/1784 [17:24<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.878\n",
            "------ 1 ------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1784/1784 [17:24<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.871\n",
            "------ 2 ------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1784/1784 [17:24<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.453\n",
            "------ 3 ------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1784/1784 [17:24<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.318\n",
            "------ 4 ------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 443/1784 [04:19<13:05,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-d78bed8ef8b2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "metadata": {
        "id": "y3QDNdkX74VE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(q, max_tokens=40):\n",
        "    model.eval()\n",
        "    tokens = vocab(tokenizer(q))\n",
        "    qna = make_qna_sequence(tokens, [])[:SEQ_LEN]\n",
        "    for i in range(max_tokens):\n",
        "        input_ = torch.tensor(qna[-SEQ_LEN:], device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            next_token = torch.argmax(model(input_)[0]).item()\n",
        "        qna.append(next_token)\n",
        "        if next_token == vocab[END]:\n",
        "            break\n",
        "    return ' '.join(vocab.lookup_tokens(qna[SEQ_LEN:]))"
      ],
      "metadata": {
        "id": "x5_0a1qK9lhz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = 'shut the fuck up'"
      ],
      "metadata": {
        "id": "d-stra3w-OAz"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask(q))"
      ],
      "metadata": {
        "id": "UzDWJlnB-RPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c935b26-2e54-423e-b03f-469716a0f539"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "erik . ! i ' m not scared . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBVu8fw7KsK_"
      },
      "execution_count": 196,
      "outputs": []
    }
  ]
}